# LookaheadRAG
Predict the retrieval “to-do list” up front with a tiny planner model, fetch everything in parallel, and let the big LLM answer in one shot—avoiding slow agentic tool loops.
